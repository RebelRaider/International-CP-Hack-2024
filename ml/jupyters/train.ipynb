{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b7dbd5d667e653",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T11:14:30.414809Z",
     "start_time": "2024-11-09T11:14:21.678015Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y imagebind\n",
    "!pip install git+https://github.com/Hackathon-Hitchhiking/ImageBind.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:35:23.571278Z",
     "start_time": "2024-11-09T10:33:07.682708Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch catboost librosa torchaudio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4375ce-e595-41fe-a9c3-b8623ded43a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install decord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac357217fab1acc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T11:19:19.134028Z",
     "start_time": "2024-11-09T11:19:15.378055Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "import librosa\n",
    "from imagebind import model\n",
    "from imagebind.model import ModalityType \n",
    "from imagebind.utils import data\n",
    "from loguru import logger\n",
    "import decord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "953bb18d973bbcc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T09:29:19.410758Z",
     "start_time": "2024-11-09T09:29:19.402700Z"
    }
   },
   "outputs": [],
   "source": [
    "class PersonalityDataset(Dataset):\n",
    "    def __init__(self, csv_file, path_to_video, imagebind_model, device):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.path_to_video = path_to_video\n",
    "        self.device = device\n",
    "\n",
    "        imagebind_model.eval()\n",
    "        imagebind_model.to(device)\n",
    "        self.imagebind_model = imagebind_model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def extract_audio_embedding(self, video_path):\n",
    "        \"\"\"\n",
    "        Извлекает аудиовектор из видео.\n",
    "        \n",
    "        Параметры\n",
    "        ----------\n",
    "        video_path : str\n",
    "            Путь к видеофайлу, из которого необходимо извлечь аудиодорожку.\n",
    "        \n",
    "        Возвращаемое значение\n",
    "        ----------------------\n",
    "        tuple\n",
    "            Кортеж, содержащий:\n",
    "            - audio : numpy.ndarray\n",
    "                Массив, представляющий аудиоданные, загруженные из временного файла.\n",
    "            - sr : int\n",
    "                Частота дискретизации аудиоданных.\n",
    "            - audio_embeddings : torch.Tensor\n",
    "                Векторное представление аудиоданных, извлеченное моделью ImageBind.\n",
    "        \n",
    "        Описание\n",
    "        ---------\n",
    "        Данная функция использует библиотеку ffmpeg для извлечения аудиодорожки из\n",
    "        видеофайла и сохранения её во временном WAV-файле. Затем с помощью библиотеки\n",
    "        librosa аудиоданные загружаются и преобразуются в массив. После этого аудиоданные\n",
    "        обрабатываются моделью ImageBind, чтобы получить векторное представление аудио.\n",
    "        Функция возвращает массив аудиоданных, частоту дискретизации и среднее значение\n",
    "        векторных представлений аудио.\n",
    "        \n",
    "        Исключения\n",
    "        ----------\n",
    "        - subprocess.CalledProcessError\n",
    "            Генерируется, если выполнение команды ffmpeg завершилось с ошибкой.\n",
    "        - FileNotFoundError\n",
    "            Генерируется, если ffmpeg или необходимые библиотеки не установлены.\n",
    "        \"\"\"\n",
    "        with tempfile.NamedTemporaryFile(suffix='.wav', delete=True) as temp_audio_file:\n",
    "            temp_audio_path = temp_audio_file.name\n",
    "            subprocess.run(\n",
    "                [\"ffmpeg\", \"-y\", \"-i\", video_path, temp_audio_path, \"-loglevel\", \"error\"],\n",
    "                check=True\n",
    "            )\n",
    "            audio, sr = librosa.load(temp_audio_path, sr=None)\n",
    "\n",
    "            inputs = {ModalityType.AUDIO: data.load_and_transform_audio_data([temp_audio_path], self.device)}\n",
    "            with torch.inference_mode():\n",
    "                audio_embeddings = self.imagebind_model(inputs)[ModalityType.AUDIO].mean(dim=0)\n",
    "\n",
    "            return audio, sr, audio_embeddings\n",
    "\n",
    "    def extract_video_embedding(self, video_path):\n",
    "        \"\"\"\n",
    "        Извлекает эмбеддинги видео, с логированием размеров каждого клипа.\n",
    "        \n",
    "        Параметры\n",
    "        ----------\n",
    "        video_path : str\n",
    "            Путь к видеофайлу, для которого необходимо извлечь эмбеддинги.\n",
    "        \n",
    "        Возвращает\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Тензор эмбеддинга видео со средним значением по всем клипам.\n",
    "            Если клипы не были извлечены, возвращается тензор нулей с размерностью (1024,).\n",
    "        \n",
    "        Примечания\n",
    "        --------\n",
    "        - Данная функция использует модель ImageBind для обработки клипов видео и извлечения эмбеддингов.\n",
    "        - Для повышения производительности используется torch.inference_mode().\n",
    "        - Если видео успешно загружается и преобразуется, каждый клип обрабатывается отдельно, после чего\n",
    "          их эмбеддинги усредняются.\n",
    "        - Если клипов нет (например, видеофайл пуст или не удалось загрузить), возвращается тензор нулей.\n",
    "        - Очищается кэш CUDA в конце для освобождения видеопамяти.\n",
    "        \n",
    "        Пример\n",
    "        -------\n",
    "        >>> video_embedding = extractor.extract_video_embedding('/path/to/video.mp4')\n",
    "        >>> print(video_embedding.shape)\n",
    "        torch.Size([1024])\n",
    "        \"\"\"\n",
    "        embeddings_list = []\n",
    "        with torch.inference_mode():\n",
    "            video_clips = data.load_and_transform_video_data([video_path], self.device)\n",
    "            if video_clips is not None:\n",
    "                # video_clips shape: (num_clips, C, T, H, W)\n",
    "                for i, video_clip in enumerate(video_clips):\n",
    "                    # Add batch dimension if necessary\n",
    "                    if video_clip.dim() == 4:\n",
    "                        video_clip = video_clip.unsqueeze(0)\n",
    "                    chunk_embeddings = self.imagebind_model({ModalityType.VISION: video_clip})\n",
    "                    embeddings_list.append(chunk_embeddings[ModalityType.VISION].mean(dim=0))\n",
    "\n",
    "        video_embedding = torch.stack(embeddings_list).mean(dim=0) if embeddings_list else torch.zeros(1024, device=self.device)\n",
    "        torch.cuda.empty_cache()\n",
    "        return video_embedding\n",
    "\n",
    "    def extract_text_embedding(self, text):\n",
    "        \"\"\"\n",
    "        Извлекает эмбеддинг для текста.\n",
    "        \n",
    "        Параметры\n",
    "        ----------\n",
    "        text : str\n",
    "            Входной текст для извлечения эмбеддинга. Если строка пустая или не является строкой,\n",
    "            будет использовано значение по умолчанию \"<UNK>\".\n",
    "        \n",
    "        Возвращает\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Эмбеддинг текста, полученный с помощью модели ImageBind.\n",
    "        \n",
    "        Примечания\n",
    "        ---------\n",
    "        Функция использует метод `load_and_transform_text` для предварительной обработки текста,\n",
    "        приводя его в формат, подходящий для обработки моделью. Затем эмбеддинг извлекается\n",
    "        в режиме inference, что позволяет выполнять вычисления без сохранения промежуточных\n",
    "        данных для обучения.\n",
    "        \n",
    "        Исключения\n",
    "        ---------\n",
    "        Проверка типа входных данных выполняется с целью обеспечения безопасности и предотвращения\n",
    "        ошибок при передаче некорректного формата. Если переданный текст не соответствует\n",
    "        ожидаемому типу, используется placeholder \"<UNK>\".\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or not text:\n",
    "            text = \"<UNK>\"\n",
    "        inputs = {ModalityType.TEXT: data.load_and_transform_text([text], self.device)}\n",
    "        with torch.inference_mode():\n",
    "            text_embedding = self.imagebind_model(inputs)[ModalityType.TEXT]\n",
    "        return text_embedding\n",
    "\n",
    "    def extract_keypoints(self, video_path, visualize=False):\n",
    "        \"\"\"\n",
    "        Извлекает ключевые точки позы, лица и рук из видео и возвращает среднее значение этих ключевых точек.\n",
    "        \n",
    "        Параметры\n",
    "        ----------\n",
    "        video_path : str\n",
    "            Путь к видеофайлу, из которого нужно извлечь ключевые точки.\n",
    "        visualize : bool, optional\n",
    "            Флаг, указывающий, нужно ли визуализировать ключевые точки на первом кадре видео (по умолчанию False).\n",
    "        \n",
    "        Возвращает\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Среднее значение ключевых точек по всем кадрам видео. Массив содержит координаты x, y, z и видимость для поз,\n",
    "            а также координаты x, y, z для лица и рук. Если в видео отсутствуют ключевые точки, возвращается массив нулей.\n",
    "        \n",
    "        Описание\n",
    "        --------\n",
    "        Функция использует библиотеку Mediapipe для анализа видео и извлечения ключевых точек человеческого тела,\n",
    "        включая позы, лицо и руки. Видео обрабатывается покадрово, и для каждого кадра вычисляются ключевые точки.\n",
    "        Если ключевые точки отсутствуют в каком-либо кадре, заполняются нулевые значения.\n",
    "        \n",
    "        В случае, если флаг `visualize` установлен в True, функция отобразит ключевые точки на первом кадре видео\n",
    "        с использованием matplotlib и OpenCV.\n",
    "        \n",
    "        Примечание\n",
    "        ---------\n",
    "        1. Функция использует Mediapipe Holistic для извлечения ключевых точек, включая улучшенные маркеры лица.\n",
    "        2. Для работы требуется установленная библиотека OpenCV (cv2), matplotlib и numpy.\n",
    "        \n",
    "        Пример\n",
    "        -------\n",
    "        >>> keypoints = self.extract_keypoints(\"path/to/video.mp4\", visualize=True)\n",
    "        >>> print(keypoints)\n",
    "        \n",
    "        \"\"\"\n",
    "        mp_holistic = mp.solutions.holistic\n",
    "        mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "        holistic = mp_holistic.Holistic(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=2,\n",
    "            enable_segmentation=False,\n",
    "            refine_face_landmarks=True\n",
    "        )\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        keypoints_list = []\n",
    "\n",
    "        frame_count = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "            # Преобразование изображения в формат RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Получение результатов\n",
    "            results = holistic.process(image)\n",
    "\n",
    "            keypoints = []\n",
    "\n",
    "            # Позы\n",
    "            if results.pose_landmarks:\n",
    "                for landmark in results.pose_landmarks.landmark:\n",
    "                    keypoints.extend([landmark.x, landmark.y,\n",
    "                                      landmark.z, landmark.visibility])\n",
    "            else:\n",
    "                keypoints.extend([0] * 33 * 4)\n",
    "\n",
    "            # Лицо\n",
    "            if results.face_landmarks:\n",
    "                for landmark in results.face_landmarks.landmark:\n",
    "                    keypoints.extend([landmark.x, landmark.y, landmark.z])\n",
    "            else:\n",
    "                keypoints.extend([0] * 468 * 3)\n",
    "\n",
    "            # Левая рука\n",
    "            if results.left_hand_landmarks:\n",
    "                for landmark in results.left_hand_landmarks.landmark:\n",
    "                    keypoints.extend([landmark.x, landmark.y, landmark.z])\n",
    "            else:\n",
    "                keypoints.extend([0] * 21 * 3)\n",
    "\n",
    "            # Правая рука\n",
    "            if results.right_hand_landmarks:\n",
    "                for landmark in results.right_hand_landmarks.landmark:\n",
    "                    keypoints.extend([landmark.x, landmark.y, landmark.z])\n",
    "            else:\n",
    "                keypoints.extend([0] * 21 * 3)\n",
    "\n",
    "            keypoints_list.append(keypoints)\n",
    "\n",
    "            if visualize and frame_count == 1:\n",
    "                # Визуализация ключевых точек на первом кадре\n",
    "                annotated_image = frame.copy()\n",
    "                # Позы\n",
    "                if results.pose_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        annotated_image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "                # Лицо\n",
    "                if results.face_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        annotated_image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)\n",
    "                # Левая рука\n",
    "                if results.left_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        annotated_image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                # Правая рука\n",
    "                if results.right_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        annotated_image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                # Преобразование изображения в формат RGB для отображения\n",
    "                annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "                # Отображение изображения в Jupyter Notebook\n",
    "                plt.figure(figsize=(10, 10))\n",
    "                plt.imshow(annotated_image)\n",
    "                plt.axis('off')\n",
    "                plt.title(f\"Ключевые точки для видео: {os.path.basename(video_path)}\")\n",
    "                plt.show()\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        holistic.close()\n",
    "\n",
    "        # Усредняем ключевые точки по всем кадрам\n",
    "        keypoints_array = np.array(keypoints_list)\n",
    "        if keypoints_array.size == 0:\n",
    "            num_keypoints = (33 * 4) + (468 * 3) + (21 * 3 * 2)\n",
    "            keypoints_mean = np.zeros(num_keypoints)\n",
    "        else:\n",
    "            keypoints_mean = keypoints_array.mean(axis=0)\n",
    "\n",
    "        return keypoints_mean\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        video_file = row['video_file']\n",
    "        video_path = os.path.join(self.path_to_video, video_file)\n",
    "        if not os.path.isfile(video_path):\n",
    "            raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "\n",
    "        logger.debug(f\"Processing video file: {video_path}\")\n",
    "\n",
    "        # Extract embeddings\n",
    "        audio, sr, audio_embeddings = self.extract_audio_embedding(video_path)\n",
    "        # video_embedding = self.extract_video_embedding(video_path)\n",
    "        transcript_embeddings = self.extract_text_embedding(row['transcript'])\n",
    "\n",
    "        key_points = self.extract_keypoints(video_path)\n",
    "\n",
    "        # Prepare labels\n",
    "        labels = {\n",
    "            'extraversion': row['extraversion'],\n",
    "            'neuroticism': row['neuroticism'],\n",
    "            'agreeableness': row['agreeableness'],\n",
    "            'conscientiousness': row['conscientiousness'],\n",
    "            'openness': row['openness'],\n",
    "            'interview': row['interview']\n",
    "        }\n",
    "\n",
    "        # Assemble all data into a dictionary\n",
    "        sample = {\n",
    "            'video_file': video_file,\n",
    "            'audio': audio,\n",
    "            'audio_embeddings': audio_embeddings,\n",
    "            'sampling_rate': sr,\n",
    "            'transcript': row['transcript'],\n",
    "            \"transcript_embeddings\": transcript_embeddings,\n",
    "            'labels': labels,\n",
    "            'key_points': key_points,\n",
    "        }\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a62a242-4905-4dbb-b4d9-60b937cf3e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/environments/hack/lib/python3.10/site-packages/imagebind/model.py:513: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\".checkpoints/imagebind_huge.pth\"))\n"
     ]
    }
   ],
   "source": [
    "imagebind_model = model.imagebind_huge(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f592522c8fa07add",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T09:29:21.585905Z",
     "start_time": "2024-11-09T09:29:21.515337Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = PersonalityDataset('processed_data/train.csv', 'processed_data/train_video', imagebind_model, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eaf588-00ff-4477-8858-fe3f0af7d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming 'PersonalityDataset' is already defined and available\n",
    "train_data = PersonalityDataset(\n",
    "    'processed_data/train.csv',\n",
    "    'processed_data/train_video',\n",
    "    imagebind_model,\n",
    "    device\n",
    ")\n",
    "\n",
    "data_loader = DataLoader(train_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c66c1-de86-4eb5-a086-0228f62fc37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = []\n",
    "\n",
    "for sample in data_loader:\n",
    "    audio_embedding = sample['audio_embeddings'].squeeze(0).cpu().numpy()\n",
    "\n",
    "    text_embedding = sample['transcript_embeddings'].squeeze(0).cpu().numpy()\n",
    "    if text_embedding.ndim > 1:\n",
    "        text_embedding = np.mean(text_embedding, axis=0)\n",
    "\n",
    "    labels = sample['labels']\n",
    "    label_values = {\n",
    "        label_name: labels[label_name].item() if isinstance(labels[label_name], torch.Tensor) else labels[label_name]\n",
    "        for label_name in [\n",
    "            'extraversion',\n",
    "            'neuroticism',\n",
    "            'agreeableness',\n",
    "            'conscientiousness',\n",
    "            'openness',\n",
    "            'interview'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    data_samples.append({\n",
    "        'audio_embedding': audio_embedding,\n",
    "        'text_embedding': text_embedding,\n",
    "        **label_values,\n",
    "        'video_id': sample['video_file'][0]\n",
    "    })\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a18084d-76e8-4e60-9def-5cb50cc860bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data_samples into a DataFrame\n",
    "df = pd.DataFrame(data_samples)\n",
    "# df.to_csv(\"embeddings.csv\", index=False)\n",
    "df.to_pickle(\"embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "247b931a-8c2d-443a-9718-1962e2f2667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "import librosa\n",
    "from imagebind import model\n",
    "from imagebind.model import ModalityType \n",
    "from imagebind.utils import data\n",
    "from loguru import logger\n",
    "import decord\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "df = pd.read_pickle(\"embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8412b6c-8c00-4a27-8855-2c84dd7fb507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the embedding columns are lists (if necessary)\n",
    "df['audio_embedding'] = df['audio_embedding'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n",
    "df['text_embedding'] = df['text_embedding'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare the data for CatBoost\n",
    "embedding_features = ['audio_embedding', 'text_embedding']\n",
    "feature_columns = embedding_features  # Since we only have embeddings as features\n",
    "\n",
    "X_train = train_df[feature_columns]\n",
    "X_test = test_df[feature_columns]\n",
    "\n",
    "y_train = train_df[['extraversion', 'neuroticism', 'agreeableness',\n",
    "                    'conscientiousness', 'openness', 'interview']]\n",
    "y_test = test_df[['extraversion', 'neuroticism', 'agreeableness',\n",
    "                  'conscientiousness', 'openness', 'interview']]\n",
    "\n",
    "label_names = ['extraversion', 'neuroticism', 'agreeableness',\n",
    "               'conscientiousness', 'openness', 'interview']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "825b5a27-28d4-4b90-aa39-81ffb817af5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for label 'extraversion'\n",
      "0:\tlearn: 0.1521773\ttest: 0.1510782\tbest: 0.1510782 (0)\ttotal: 1.74ms\tremaining: 1.74s\n",
      "100:\tlearn: 0.1357652\ttest: 0.1307827\tbest: 0.1307827 (100)\ttotal: 93.9ms\tremaining: 836ms\n",
      "200:\tlearn: 0.1322818\ttest: 0.1262011\tbest: 0.1262011 (200)\ttotal: 202ms\tremaining: 804ms\n",
      "300:\tlearn: 0.1310815\ttest: 0.1250740\tbest: 0.1250740 (300)\ttotal: 291ms\tremaining: 675ms\n",
      "400:\tlearn: 0.1303153\ttest: 0.1247184\tbest: 0.1247152 (394)\ttotal: 375ms\tremaining: 560ms\n",
      "500:\tlearn: 0.1296823\ttest: 0.1245797\tbest: 0.1245797 (500)\ttotal: 461ms\tremaining: 459ms\n",
      "600:\tlearn: 0.1291033\ttest: 0.1245320\tbest: 0.1245319 (591)\ttotal: 546ms\tremaining: 362ms\n",
      "700:\tlearn: 0.1285250\ttest: 0.1245245\tbest: 0.1245129 (647)\ttotal: 633ms\tremaining: 270ms\n",
      "800:\tlearn: 0.1280001\ttest: 0.1245552\tbest: 0.1245129 (647)\ttotal: 718ms\tremaining: 178ms\n",
      "900:\tlearn: 0.1274340\ttest: 0.1245735\tbest: 0.1245129 (647)\ttotal: 805ms\tremaining: 88.4ms\n",
      "999:\tlearn: 0.1268635\ttest: 0.1246185\tbest: 0.1245129 (647)\ttotal: 890ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.1245128634\n",
      "bestIteration = 647\n",
      "\n",
      "Shrink model to first 648 iterations.\n",
      "Training model for label 'neuroticism'\n",
      "0:\tlearn: 0.1537779\ttest: 0.1509856\tbest: 0.1509856 (0)\ttotal: 1.12ms\tremaining: 1.12s\n",
      "100:\tlearn: 0.1387007\ttest: 0.1331109\tbest: 0.1331109 (100)\ttotal: 82.6ms\tremaining: 735ms\n",
      "200:\tlearn: 0.1354483\ttest: 0.1290955\tbest: 0.1290955 (200)\ttotal: 163ms\tremaining: 646ms\n",
      "300:\tlearn: 0.1342617\ttest: 0.1280689\tbest: 0.1280689 (300)\ttotal: 242ms\tremaining: 563ms\n",
      "400:\tlearn: 0.1335038\ttest: 0.1276931\tbest: 0.1276931 (400)\ttotal: 320ms\tremaining: 477ms\n",
      "500:\tlearn: 0.1328821\ttest: 0.1275643\tbest: 0.1275638 (493)\ttotal: 398ms\tremaining: 397ms\n",
      "600:\tlearn: 0.1323076\ttest: 0.1274850\tbest: 0.1274744 (592)\ttotal: 484ms\tremaining: 321ms\n",
      "700:\tlearn: 0.1317518\ttest: 0.1275015\tbest: 0.1274744 (592)\ttotal: 586ms\tremaining: 250ms\n",
      "800:\tlearn: 0.1311908\ttest: 0.1275036\tbest: 0.1274744 (592)\ttotal: 664ms\tremaining: 165ms\n",
      "900:\tlearn: 0.1306016\ttest: 0.1275254\tbest: 0.1274744 (592)\ttotal: 745ms\tremaining: 81.9ms\n",
      "999:\tlearn: 0.1299871\ttest: 0.1274875\tbest: 0.1274744 (592)\ttotal: 822ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.1274744045\n",
      "bestIteration = 592\n",
      "\n",
      "Shrink model to first 593 iterations.\n",
      "Training model for label 'agreeableness'\n",
      "0:\tlearn: 0.1368908\ttest: 0.1336789\tbest: 0.1336789 (0)\ttotal: 1.17ms\tremaining: 1.17s\n",
      "100:\tlearn: 0.1300907\ttest: 0.1239125\tbest: 0.1239125 (100)\ttotal: 98.7ms\tremaining: 878ms\n",
      "200:\tlearn: 0.1282434\ttest: 0.1212962\tbest: 0.1212962 (200)\ttotal: 192ms\tremaining: 765ms\n",
      "300:\tlearn: 0.1273124\ttest: 0.1205349\tbest: 0.1205349 (300)\ttotal: 292ms\tremaining: 677ms\n",
      "400:\tlearn: 0.1266222\ttest: 0.1202669\tbest: 0.1202650 (399)\ttotal: 371ms\tremaining: 554ms\n",
      "500:\tlearn: 0.1260124\ttest: 0.1201593\tbest: 0.1201531 (495)\ttotal: 448ms\tremaining: 446ms\n",
      "600:\tlearn: 0.1254473\ttest: 0.1201877\tbest: 0.1201531 (495)\ttotal: 524ms\tremaining: 348ms\n",
      "700:\tlearn: 0.1248749\ttest: 0.1201875\tbest: 0.1201531 (495)\ttotal: 603ms\tremaining: 257ms\n",
      "800:\tlearn: 0.1242938\ttest: 0.1202036\tbest: 0.1201531 (495)\ttotal: 679ms\tremaining: 169ms\n",
      "900:\tlearn: 0.1236926\ttest: 0.1202736\tbest: 0.1201531 (495)\ttotal: 756ms\tremaining: 83.1ms\n",
      "999:\tlearn: 0.1230643\ttest: 0.1203101\tbest: 0.1201531 (495)\ttotal: 834ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.1201530576\n",
      "bestIteration = 495\n",
      "\n",
      "Shrink model to first 496 iterations.\n",
      "Training model for label 'conscientiousness'\n",
      "0:\tlearn: 0.1555354\ttest: 0.1518912\tbest: 0.1518912 (0)\ttotal: 2.44ms\tremaining: 2.44s\n",
      "100:\tlearn: 0.1369602\ttest: 0.1296987\tbest: 0.1296987 (100)\ttotal: 78.8ms\tremaining: 701ms\n",
      "200:\tlearn: 0.1330111\ttest: 0.1250800\tbest: 0.1250800 (200)\ttotal: 156ms\tremaining: 621ms\n",
      "300:\tlearn: 0.1317249\ttest: 0.1239423\tbest: 0.1239423 (300)\ttotal: 240ms\tremaining: 557ms\n",
      "400:\tlearn: 0.1309151\ttest: 0.1236571\tbest: 0.1236472 (398)\ttotal: 320ms\tremaining: 478ms\n",
      "500:\tlearn: 0.1302644\ttest: 0.1235891\tbest: 0.1235784 (482)\ttotal: 398ms\tremaining: 397ms\n",
      "600:\tlearn: 0.1296786\ttest: 0.1235460\tbest: 0.1235460 (600)\ttotal: 478ms\tremaining: 317ms\n",
      "700:\tlearn: 0.1290942\ttest: 0.1235614\tbest: 0.1235255 (654)\ttotal: 557ms\tremaining: 237ms\n",
      "800:\tlearn: 0.1285184\ttest: 0.1235829\tbest: 0.1235255 (654)\ttotal: 632ms\tremaining: 157ms\n",
      "900:\tlearn: 0.1279339\ttest: 0.1236248\tbest: 0.1235255 (654)\ttotal: 709ms\tremaining: 77.9ms\n",
      "999:\tlearn: 0.1272950\ttest: 0.1236542\tbest: 0.1235255 (654)\ttotal: 787ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.1235255458\n",
      "bestIteration = 654\n",
      "\n",
      "Shrink model to first 655 iterations.\n",
      "Training model for label 'openness'\n",
      "0:\tlearn: 0.1469346\ttest: 0.1455756\tbest: 0.1455756 (0)\ttotal: 1.53ms\tremaining: 1.53s\n",
      "100:\tlearn: 0.1314613\ttest: 0.1276557\tbest: 0.1276557 (100)\ttotal: 135ms\tremaining: 1.2s\n",
      "200:\tlearn: 0.1281862\ttest: 0.1236861\tbest: 0.1236861 (200)\ttotal: 272ms\tremaining: 1.08s\n",
      "300:\tlearn: 0.1270625\ttest: 0.1226247\tbest: 0.1226247 (300)\ttotal: 361ms\tremaining: 839ms\n",
      "400:\tlearn: 0.1263529\ttest: 0.1223016\tbest: 0.1223016 (400)\ttotal: 440ms\tremaining: 657ms\n",
      "500:\tlearn: 0.1257881\ttest: 0.1222150\tbest: 0.1222150 (500)\ttotal: 531ms\tremaining: 528ms\n",
      "600:\tlearn: 0.1252414\ttest: 0.1222013\tbest: 0.1221898 (583)\ttotal: 635ms\tremaining: 422ms\n",
      "700:\tlearn: 0.1247253\ttest: 0.1221889\tbest: 0.1221729 (674)\ttotal: 719ms\tremaining: 307ms\n",
      "800:\tlearn: 0.1241765\ttest: 0.1221993\tbest: 0.1221729 (674)\ttotal: 799ms\tremaining: 199ms\n",
      "900:\tlearn: 0.1236240\ttest: 0.1222390\tbest: 0.1221729 (674)\ttotal: 878ms\tremaining: 96.4ms\n",
      "999:\tlearn: 0.1230329\ttest: 0.1222870\tbest: 0.1221729 (674)\ttotal: 954ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.122172863\n",
      "bestIteration = 674\n",
      "\n",
      "Shrink model to first 675 iterations.\n",
      "Training model for label 'interview'\n",
      "0:\tlearn: 0.1498602\ttest: 0.1497921\tbest: 0.1497921 (0)\ttotal: 1.26ms\tremaining: 1.26s\n",
      "100:\tlearn: 0.1350838\ttest: 0.1320933\tbest: 0.1320933 (100)\ttotal: 103ms\tremaining: 920ms\n",
      "200:\tlearn: 0.1318703\ttest: 0.1281377\tbest: 0.1281377 (200)\ttotal: 237ms\tremaining: 944ms\n",
      "300:\tlearn: 0.1307317\ttest: 0.1271019\tbest: 0.1271019 (300)\ttotal: 348ms\tremaining: 808ms\n",
      "400:\tlearn: 0.1300042\ttest: 0.1268060\tbest: 0.1268060 (400)\ttotal: 427ms\tremaining: 638ms\n",
      "500:\tlearn: 0.1294004\ttest: 0.1266665\tbest: 0.1266665 (500)\ttotal: 510ms\tremaining: 508ms\n",
      "600:\tlearn: 0.1288140\ttest: 0.1265696\tbest: 0.1265696 (600)\ttotal: 592ms\tremaining: 393ms\n",
      "700:\tlearn: 0.1282879\ttest: 0.1265452\tbest: 0.1265413 (695)\ttotal: 672ms\tremaining: 287ms\n",
      "800:\tlearn: 0.1277270\ttest: 0.1265727\tbest: 0.1265408 (715)\ttotal: 750ms\tremaining: 186ms\n",
      "900:\tlearn: 0.1271373\ttest: 0.1266235\tbest: 0.1265408 (715)\ttotal: 829ms\tremaining: 91.1ms\n",
      "999:\tlearn: 0.1265136\ttest: 0.1266785\tbest: 0.1265408 (715)\ttotal: 910ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.1265407824\n",
      "bestIteration = 715\n",
      "\n",
      "Shrink model to first 716 iterations.\n"
     ]
    }
   ],
   "source": [
    "regressors = {}\n",
    "\n",
    "for label_name in label_names:\n",
    "    print(f\"Training model for label '{label_name}'\")\n",
    "    train_pool = Pool(\n",
    "        data=X_train,\n",
    "        label=y_train[label_name],\n",
    "        embedding_features=embedding_features\n",
    "    )\n",
    "    test_pool = Pool(\n",
    "        data=X_test,\n",
    "        label=y_test[label_name],\n",
    "        embedding_features=embedding_features\n",
    "    )\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=1000,\n",
    "        learning_rate=0.01,\n",
    "        depth=6,\n",
    "        loss_function='RMSE',\n",
    "        eval_metric='RMSE',\n",
    "        random_seed=42,\n",
    "        verbose=100\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        train_pool,\n",
    "        eval_set=test_pool,\n",
    "        use_best_model=True\n",
    "    )\n",
    "\n",
    "    regressors[label_name] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86d31c65-af7c-453d-a4c7-becc7aa2f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {label_name: {} for label_name in label_names}\n",
    "ground_truths = {label_name: {} for label_name in label_names}\n",
    "\n",
    "for idx in train_df.index:\n",
    "    x = train_df.loc[[idx], feature_columns]\n",
    "    video_id = train_df.loc[idx, 'video_id']\n",
    "\n",
    "    sample_pool = Pool(\n",
    "        data=x,\n",
    "        embedding_features=embedding_features\n",
    "    )\n",
    "\n",
    "    for label_name in label_names:\n",
    "        model = regressors[label_name]\n",
    "        y_pred = model.predict(sample_pool)[0]\n",
    "        y_true = y_train.loc[idx, label_name]\n",
    "\n",
    "        predictions[label_name][video_id] = y_pred\n",
    "        ground_truths[label_name][video_id] = y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8bfc0f3-de1b-4493-abbb-d9de4d342af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Average F1 score across all labels: 0.3933\n"
     ]
    }
   ],
   "source": [
    "def calculate_tag_f1(pred: dict, truth: dict):\n",
    "    epsilon = 0.1\n",
    "    predicts = []\n",
    "    truths = []\n",
    "    for pred_key in list(pred.keys()):\n",
    "        predict_value = pred[pred_key]\n",
    "        if pred_key in list(truth.keys()):\n",
    "            truth_value = truth[pred_key]\n",
    "        else:\n",
    "            raise Exception(f'Видеозапись с названием {pred_key} отсутствует в датасете!')\n",
    "        predicts.append(1 if abs(predict_value - truth_value) < epsilon else 0)\n",
    "        truths.append(1)\n",
    "\n",
    "    while len(predicts) < len(list(truth.keys())):\n",
    "        predicts.append(0)\n",
    "\n",
    "    while len(truths) < len(list(truth.keys())):\n",
    "        truths.append(1)\n",
    "\n",
    "    return f1_score(truths, predicts, average='macro')\n",
    "\n",
    "\n",
    "def calculate_f1(pred, truth):\n",
    "    results = []\n",
    "\n",
    "    extraversion_pred_raw = pred['extraversion']\n",
    "    extraversion_truth_raw = truth['extraversion']\n",
    "    results.append(calculate_tag_f1(extraversion_pred_raw, extraversion_truth_raw))\n",
    "\n",
    "    neuroticism_pred_raw = pred['neuroticism']\n",
    "    neuroticism_truth_raw = truth['neuroticism']\n",
    "    results.append(calculate_tag_f1(neuroticism_pred_raw, neuroticism_truth_raw))\n",
    "\n",
    "    agreeableness_pred_raw = pred['agreeableness']\n",
    "    agreeableness_truth_raw = truth['agreeableness']\n",
    "    results.append(calculate_tag_f1(agreeableness_pred_raw, agreeableness_truth_raw))\n",
    "\n",
    "    conscientiousness_pred_raw = pred['conscientiousness']\n",
    "    conscientiousness_truth_raw = truth['conscientiousness']\n",
    "    results.append(calculate_tag_f1(conscientiousness_pred_raw, conscientiousness_truth_raw))\n",
    "\n",
    "    openness_pred_raw = pred['openness']\n",
    "    openness_truth_raw = truth['openness']\n",
    "    results.append(calculate_tag_f1(openness_pred_raw, openness_truth_raw))\n",
    "\n",
    "    return np.mean(results)\n",
    "\n",
    "average_f1 = calculate_f1(predictions, ground_truths)\n",
    "print(f\"\\nFinal Average F1 score across all labels: {average_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
