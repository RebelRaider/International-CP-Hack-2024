{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.utils.data import Dataset\n",
    "import tempfile\n",
    "import subprocess\n",
    "import librosa\n",
    "from imagebind.model import ModalityType\n",
    "from imagebind.utils import data\n",
    "from loguru import logger\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ],
   "id": "6b709e5bf89ae9c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PersonalityDataset(Dataset):\n",
    "    def __init__(self, csv_file, path_to_video, imagebind_model, device):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.path_to_video = path_to_video\n",
    "        self.device = device\n",
    "\n",
    "        imagebind_model.eval()\n",
    "        imagebind_model.to(device)\n",
    "        self.imagebind_model = imagebind_model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def extract_audio_embedding(self, video_path):\n",
    "        \"\"\"\n",
    "        Извлекает аудиовектор из видео.\n",
    "        \n",
    "        Параметры\n",
    "        ----------\n",
    "        video_path : str\n",
    "            Путь к видеофайлу, из которого необходимо извлечь аудиодорожку.\n",
    "        \n",
    "        Возвращаемое значение\n",
    "        ----------------------\n",
    "        tuple\n",
    "            Кортеж, содержащий:\n",
    "            - audio : numpy.ndarray\n",
    "                Массив, представляющий аудиоданные, загруженные из временного файла.\n",
    "            - sr : int\n",
    "                Частота дискретизации аудиоданных.\n",
    "            - audio_embeddings : torch.Tensor\n",
    "                Векторное представление аудиоданных, извлеченное моделью ImageBind.\n",
    "        \n",
    "        Описание\n",
    "        ---------\n",
    "        Данная функция использует библиотеку ffmpeg для извлечения аудиодорожки из\n",
    "        видеофайла и сохранения её во временном WAV-файле. Затем с помощью библиотеки\n",
    "        librosa аудиоданные загружаются и преобразуются в массив. После этого аудиоданные\n",
    "        обрабатываются моделью ImageBind, чтобы получить векторное представление аудио.\n",
    "        Функция возвращает массив аудиоданных, частоту дискретизации и среднее значение\n",
    "        векторных представлений аудио.\n",
    "        \n",
    "        Исключения\n",
    "        ----------\n",
    "        - subprocess.CalledProcessError\n",
    "            Генерируется, если выполнение команды ffmpeg завершилось с ошибкой.\n",
    "        - FileNotFoundError\n",
    "            Генерируется, если ffmpeg или необходимые библиотеки не установлены.\n",
    "        \"\"\"\n",
    "        with tempfile.NamedTemporaryFile(suffix='.wav', delete=True) as temp_audio_file:\n",
    "            temp_audio_path = temp_audio_file.name\n",
    "            subprocess.run(\n",
    "                [\"ffmpeg\", \"-y\", \"-i\", video_path, temp_audio_path, \"-loglevel\", \"error\"],\n",
    "                check=True\n",
    "            )\n",
    "            audio, sr = librosa.load(temp_audio_path, sr=None)\n",
    "\n",
    "            inputs = {ModalityType.AUDIO: data.load_and_transform_audio_data([temp_audio_path], self.device)}\n",
    "            with torch.inference_mode():\n",
    "                audio_embeddings = self.imagebind_model(inputs)[ModalityType.AUDIO].mean(dim=0)\n",
    "\n",
    "            return audio, sr, audio_embeddings\n",
    "\n",
    "    def extract_video_embedding(self, video_path):\n",
    "        \"\"\"\n",
    "        Извлекает эмбеддинги видео, с логированием размеров каждого клипа.\n",
    "        \n",
    "        Параметры\n",
    "        ----------\n",
    "        video_path : str\n",
    "            Путь к видеофайлу, для которого необходимо извлечь эмбеддинги.\n",
    "        \n",
    "        Возвращает\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Тензор эмбеддинга видео со средним значением по всем клипам.\n",
    "            Если клипы не были извлечены, возвращается тензор нулей с размерностью (1024,).\n",
    "        \n",
    "        Примечания\n",
    "        --------\n",
    "        - Данная функция использует модель ImageBind для обработки клипов видео и извлечения эмбеддингов.\n",
    "        - Для повышения производительности используется torch.inference_mode().\n",
    "        - Если видео успешно загружается и преобразуется, каждый клип обрабатывается отдельно, после чего\n",
    "          их эмбеддинги усредняются.\n",
    "        - Если клипов нет (например, видеофайл пуст или не удалось загрузить), возвращается тензор нулей.\n",
    "        - Очищается кэш CUDA в конце для освобождения видеопамяти.\n",
    "        \n",
    "        Пример\n",
    "        -------\n",
    "        >>> video_embedding = extractor.extract_video_embedding('/path/to/video.mp4')\n",
    "        >>> print(video_embedding.shape)\n",
    "        torch.Size([1024])\n",
    "        \"\"\"\n",
    "        embeddings_list = []\n",
    "        with torch.inference_mode():\n",
    "            video_clips = data.load_and_transform_video_data([video_path], self.device)\n",
    "            if video_clips is not None:\n",
    "                # video_clips shape: (num_clips, C, T, H, W)\n",
    "                for i, video_clip in enumerate(video_clips):\n",
    "                    # Add batch dimension if necessary\n",
    "                    if video_clip.dim() == 4:\n",
    "                        video_clip = video_clip.unsqueeze(0)\n",
    "                    chunk_embeddings = self.imagebind_model({ModalityType.VISION: video_clip})\n",
    "                    embeddings_list.append(chunk_embeddings[ModalityType.VISION].mean(dim=0))\n",
    "\n",
    "        video_embedding = torch.stack(embeddings_list).mean(dim=0) if embeddings_list else torch.zeros(1024, device=self.device)\n",
    "        torch.cuda.empty_cache()\n",
    "        return video_embedding\n",
    "\n",
    "    def extract_text_embedding(self, text):\n",
    "        \"\"\"\n",
    "        Извлекает эмбеддинг для текста.\n",
    "        \n",
    "        Параметры\n",
    "        ----------\n",
    "        text : str\n",
    "            Входной текст для извлечения эмбеддинга. Если строка пустая или не является строкой,\n",
    "            будет использовано значение по умолчанию \"<UNK>\".\n",
    "        \n",
    "        Возвращает\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Эмбеддинг текста, полученный с помощью модели ImageBind.\n",
    "        \n",
    "        Примечания\n",
    "        ---------\n",
    "        Функция использует метод `load_and_transform_text` для предварительной обработки текста,\n",
    "        приводя его в формат, подходящий для обработки моделью. Затем эмбеддинг извлекается\n",
    "        в режиме inference, что позволяет выполнять вычисления без сохранения промежуточных\n",
    "        данных для обучения.\n",
    "        \n",
    "        Исключения\n",
    "        ---------\n",
    "        Проверка типа входных данных выполняется с целью обеспечения безопасности и предотвращения\n",
    "        ошибок при передаче некорректного формата. Если переданный текст не соответствует\n",
    "        ожидаемому типу, используется placeholder \"<UNK>\".\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or not text:\n",
    "            text = \"<UNK>\"\n",
    "        inputs = {ModalityType.TEXT: data.load_and_transform_text([text], self.device)}\n",
    "        with torch.inference_mode():\n",
    "            text_embedding = self.imagebind_model(inputs)[ModalityType.TEXT]\n",
    "        return text_embedding\n",
    "\n",
    "    def extract_keypoints(self, video_path, visualize=False):\n",
    "        \"\"\"\n",
    "        Извлекает ключевые точки позы, лица и рук из видео и возвращает среднее значение этих ключевых точек.\n",
    "        \n",
    "        Параметры\n",
    "        ----------\n",
    "        video_path : str\n",
    "            Путь к видеофайлу, из которого нужно извлечь ключевые точки.\n",
    "        visualize : bool, optional\n",
    "            Флаг, указывающий, нужно ли визуализировать ключевые точки на первом кадре видео (по умолчанию False).\n",
    "        \n",
    "        Возвращает\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Среднее значение ключевых точек по всем кадрам видео. Массив содержит координаты x, y, z и видимость для поз,\n",
    "            а также координаты x, y, z для лица и рук. Если в видео отсутствуют ключевые точки, возвращается массив нулей.\n",
    "        \n",
    "        Описание\n",
    "        --------\n",
    "        Функция использует библиотеку Mediapipe для анализа видео и извлечения ключевых точек человеческого тела,\n",
    "        включая позы, лицо и руки. Видео обрабатывается покадрово, и для каждого кадра вычисляются ключевые точки.\n",
    "        Если ключевые точки отсутствуют в каком-либо кадре, заполняются нулевые значения.\n",
    "        \n",
    "        В случае, если флаг `visualize` установлен в True, функция отобразит ключевые точки на первом кадре видео\n",
    "        с использованием matplotlib и OpenCV.\n",
    "        \n",
    "        Примечание\n",
    "        ---------\n",
    "        1. Функция использует Mediapipe Holistic для извлечения ключевых точек, включая улучшенные маркеры лица.\n",
    "        2. Для работы требуется установленная библиотека OpenCV (cv2), matplotlib и numpy.\n",
    "        \n",
    "        Пример\n",
    "        -------\n",
    "        >>> keypoints = self.extract_keypoints(\"path/to/video.mp4\", visualize=True)\n",
    "        >>> print(keypoints)\n",
    "        \n",
    "        \"\"\"\n",
    "        mp_holistic = mp.solutions.holistic\n",
    "        mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "        holistic = mp_holistic.Holistic(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=2,\n",
    "            enable_segmentation=False,\n",
    "            refine_face_landmarks=True\n",
    "        )\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        keypoints_list = []\n",
    "\n",
    "        frame_count = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "            # Преобразование изображения в формат RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Получение результатов\n",
    "            results = holistic.process(image)\n",
    "\n",
    "            keypoints = []\n",
    "\n",
    "            # Позы\n",
    "            if results.pose_landmarks:\n",
    "                for landmark in results.pose_landmarks.landmark:\n",
    "                    keypoints.extend([landmark.x, landmark.y,\n",
    "                                      landmark.z, landmark.visibility])\n",
    "            else:\n",
    "                keypoints.extend([0] * 33 * 4)\n",
    "\n",
    "            # Лицо\n",
    "            if results.face_landmarks:\n",
    "                for landmark in results.face_landmarks.landmark:\n",
    "                    keypoints.extend([landmark.x, landmark.y, landmark.z])\n",
    "            else:\n",
    "                keypoints.extend([0] * 468 * 3)\n",
    "\n",
    "            # Левая рука\n",
    "            if results.left_hand_landmarks:\n",
    "                for landmark in results.left_hand_landmarks.landmark:\n",
    "                    keypoints.extend([landmark.x, landmark.y, landmark.z])\n",
    "            else:\n",
    "                keypoints.extend([0] * 21 * 3)\n",
    "\n",
    "            # Правая рука\n",
    "            if results.right_hand_landmarks:\n",
    "                for landmark in results.right_hand_landmarks.landmark:\n",
    "                    keypoints.extend([landmark.x, landmark.y, landmark.z])\n",
    "            else:\n",
    "                keypoints.extend([0] * 21 * 3)\n",
    "\n",
    "            keypoints_list.append(keypoints)\n",
    "\n",
    "            if visualize and frame_count == 1:\n",
    "                # Визуализация ключевых точек на первом кадре\n",
    "                annotated_image = frame.copy()\n",
    "                # Позы\n",
    "                if results.pose_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        annotated_image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "                # Лицо\n",
    "                if results.face_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        annotated_image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)\n",
    "                # Левая рука\n",
    "                if results.left_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        annotated_image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                # Правая рука\n",
    "                if results.right_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        annotated_image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                # Преобразование изображения в формат RGB для отображения\n",
    "                annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "                # Отображение изображения в Jupyter Notebook\n",
    "                plt.figure(figsize=(10, 10))\n",
    "                plt.imshow(annotated_image)\n",
    "                plt.axis('off')\n",
    "                plt.title(f\"Ключевые точки для видео: {os.path.basename(video_path)}\")\n",
    "                plt.show()\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        holistic.close()\n",
    "\n",
    "        # Усредняем ключевые точки по всем кадрам\n",
    "        keypoints_array = np.array(keypoints_list)\n",
    "        if keypoints_array.size == 0:\n",
    "            num_keypoints = (33 * 4) + (468 * 3) + (21 * 3 * 2)\n",
    "            keypoints_mean = np.zeros(num_keypoints)\n",
    "        else:\n",
    "            keypoints_mean = keypoints_array.mean(axis=0)\n",
    "\n",
    "        return keypoints_mean\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        video_file = row['video_file']\n",
    "        video_path = os.path.join(self.path_to_video, video_file)\n",
    "        if not os.path.isfile(video_path):\n",
    "            raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "\n",
    "        logger.debug(f\"Processing video file: {video_path}\")\n",
    "\n",
    "        # Extract embeddings\n",
    "        audio, sr, audio_embeddings = self.extract_audio_embedding(video_path)\n",
    "        # video_embedding = self.extract_video_embedding(video_path)\n",
    "        transcript_embeddings = self.extract_text_embedding(row['transcript'])\n",
    "        \n",
    "        key_points = self.extract_keypoints(video_path)\n",
    "        \n",
    "        # Prepare labels\n",
    "        labels = {\n",
    "            'extraversion': row['extraversion'],\n",
    "            'neuroticism': row['neuroticism'],\n",
    "            'agreeableness': row['agreeableness'],\n",
    "            'conscientiousness': row['conscientiousness'],\n",
    "            'openness': row['openness'],\n",
    "            'interview': row['interview']\n",
    "        }\n",
    "\n",
    "        # Assemble all data into a dictionary\n",
    "        sample = {\n",
    "            'video_file': video_file,\n",
    "            'audio': audio,\n",
    "            'audio_embeddings': audio_embeddings,\n",
    "            'sampling_rate': sr,\n",
    "            'transcript': row['transcript'],\n",
    "            \"transcript_embeddings\": transcript_embeddings,\n",
    "            'labels': labels,\n",
    "            'key_points': key_points,\n",
    "        }\n",
    "\n",
    "        return sample\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
