{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dde6caf-7467-4c87-af0c-8ec46bbb93a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/user1/environments/hack/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/user1/environments/hack/lib/python3.10/site-packages (1.26.4)\n",
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.1/36.1 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /home/user1/environments/hack/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: catboost in /home/user1/environments/hack/lib/python3.10/site-packages (1.2.7)\n",
      "Requirement already satisfied: transformers in /home/user1/environments/hack/lib/python3.10/site-packages (4.46.2)\n",
      "Requirement already satisfied: librosa in /home/user1/environments/hack/lib/python3.10/site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: opencv-python in /home/user1/environments/hack/lib/python3.10/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: torch in /home/user1/environments/hack/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/user1/environments/hack/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/user1/environments/hack/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/user1/environments/hack/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting jaxlib\n",
      "  Downloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl (87.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 KB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /home/user1/environments/hack/lib/python3.10/site-packages (from mediapipe) (3.9.2)\n",
      "Collecting protobuf<5,>=4.25.3\n",
      "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sounddevice>=0.4.4\n",
      "  Downloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=19.1.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from mediapipe) (24.2.0)\n",
      "Collecting jax\n",
      "  Downloading jax-0.4.35-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Collecting opencv-contrib-python\n",
      "  Downloading opencv_contrib_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (68.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=3.1.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: six in /home/user1/environments/hack/lib/python3.10/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: plotly in /home/user1/environments/hack/lib/python3.10/site-packages (from catboost) (5.24.1)\n",
      "Requirement already satisfied: graphviz in /home/user1/environments/hack/lib/python3.10/site-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: requests in /home/user1/environments/hack/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/user1/environments/hack/lib/python3.10/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/user1/environments/hack/lib/python3.10/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/user1/environments/hack/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/user1/environments/hack/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: filelock in /home/user1/environments/hack/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/user1/environments/hack/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user1/environments/hack/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/user1/environments/hack/lib/python3.10/site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /home/user1/environments/hack/lib/python3.10/site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/user1/environments/hack/lib/python3.10/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /home/user1/environments/hack/lib/python3.10/site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/user1/environments/hack/lib/python3.10/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /home/user1/environments/hack/lib/python3.10/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: fsspec in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: networkx in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/user1/environments/hack/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user1/environments/hack/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user1/environments/hack/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user1/environments/hack/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user1/environments/hack/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: CFFI>=1.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Collecting opt-einsum\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes>=0.4.0\n",
      "  Downloading ml_dtypes-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/user1/environments/hack/lib/python3.10/site-packages (from matplotlib->mediapipe) (11.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/user1/environments/hack/lib/python3.10/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/user1/environments/hack/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.4.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/user1/environments/hack/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/user1/environments/hack/lib/python3.10/site-packages (from matplotlib->mediapipe) (3.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from matplotlib->mediapipe) (4.54.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from plotly->catboost) (9.0.0)\n",
      "Requirement already satisfied: pycparser in /home/user1/environments/hack/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Installing collected packages: sentencepiece, flatbuffers, protobuf, opt-einsum, opencv-contrib-python, ml-dtypes, absl-py, sounddevice, jaxlib, jax, mediapipe\n",
      "Successfully installed absl-py-2.1.0 flatbuffers-24.3.25 jax-0.4.35 jaxlib-0.4.35 mediapipe-0.10.18 ml-dtypes-0.5.0 opencv-contrib-python-4.10.0.84 opt-einsum-3.4.0 protobuf-4.25.5 sentencepiece-0.2.0 sounddevice-0.5.1\n",
      "Requirement already satisfied: torch in /home/user1/environments/hack/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /home/user1/environments/hack/lib/python3.10/site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in /home/user1/environments/hack/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: fsspec in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: jinja2 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: filelock in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/user1/environments/hack/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/user1/environments/hack/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/user1/environments/hack/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy mediapipe scikit-learn catboost transformers librosa opencv-python torch\n",
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b46fd93-5c3b-4eb7-9eb0-1fdf72244708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731181032.370605  279224 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1731181032.383314  361918 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 550.127.05), renderer: Tesla V100-SXM3-32GB/PCIe/SSE2\n",
      "W0000 00:00:1731181032.549271  361914 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181032.694178  361917 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181032.696663  361916 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181032.697197  361914 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181032.701098  361917 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181032.713247  361916 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181032.722982  361917 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181032.727820  361914 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1731181070.245176  279224 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1731181070.257047  362113 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 550.127.05), renderer: Tesla V100-SXM3-32GB/PCIe/SSE2\n",
      "W0000 00:00:1731181070.329149  362112 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181070.433643  362111 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181070.435527  362109 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181070.435761  362111 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181070.437300  362109 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181070.448730  362109 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181070.450537  362112 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181070.455240  362111 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1731181106.987272  279224 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1731181106.999447  362299 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 550.127.05), renderer: Tesla V100-SXM3-32GB/PCIe/SSE2\n",
      "W0000 00:00:1731181107.098749  362298 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181107.215664  362295 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181107.218033  362297 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181107.219680  362296 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181107.221143  362295 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181107.231946  362297 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181107.244343  362295 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181107.247475  362296 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1731181141.280818  279224 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1731181141.293583  362481 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 550.127.05), renderer: Tesla V100-SXM3-32GB/PCIe/SSE2\n",
      "W0000 00:00:1731181141.392827  362477 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181141.531574  362480 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181141.534175  362479 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181141.536176  362477 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181141.536428  362480 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181141.546227  362479 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181141.562283  362477 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181141.566149  362480 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1731181180.262914  279224 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1731181180.274896  362649 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 550.127.05), renderer: Tesla V100-SXM3-32GB/PCIe/SSE2\n",
      "W0000 00:00:1731181180.356854  362648 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181180.473783  362645 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181180.475900  362648 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181180.478227  362647 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181180.481001  362645 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181180.489687  362648 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181180.502208  362647 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181180.512061  362645 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1731181218.169114  279224 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1731181218.181057  362825 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 550.127.05), renderer: Tesla V100-SXM3-32GB/PCIe/SSE2\n",
      "W0000 00:00:1731181218.271603  362823 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181218.400749  362824 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181218.402811  362823 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181218.403149  362821 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181218.402852  362822 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181218.415516  362823 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181218.422639  362822 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731181218.423471  362821 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (459,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 131\u001b[0m\n\u001b[1;32m    128\u001b[0m y \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    129\u001b[0m video_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m    132\u001b[0m     keypoints \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    133\u001b[0m     labels \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/environments/hack/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/environments/hack/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/environments/hack/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/environments/hack/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[5], line 103\u001b[0m, in \u001b[0;36mPersonalityDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Извлечение ключевых точек\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_keypoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Подготовка меток\u001b[39;00m\n\u001b[1;32m    106\u001b[0m labels \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[5], line 86\u001b[0m, in \u001b[0;36mPersonalityDataset.extract_keypoints\u001b[0;34m(self, video_path)\u001b[0m\n\u001b[1;32m     83\u001b[0m holistic\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Усредняем ключевые точки по всем кадрам\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m keypoints_array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeypoints_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keypoints_array\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     88\u001b[0m     num_keypoints \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m33\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m468\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m21\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (459,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from catboost import CatBoostRegressor\n",
    "import torch\n",
    "\n",
    "# Определяем названия меток\n",
    "label_names = ['extraversion', 'neuroticism', 'agreeableness',\n",
    "               'conscientiousness', 'openness']\n",
    "\n",
    "class PersonalityDataset:\n",
    "    def __init__(self, csv_file, path_to_video):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.path_to_video = path_to_video\n",
    "        self.label_names = label_names\n",
    "        # Ограничиваем датасет первыми 30 видео\n",
    "        self.data = self.data.iloc[:30]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def extract_keypoints(self, video_path, visualize=False):\n",
    "        mp_holistic = mp.solutions.holistic\n",
    "        mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "        holistic = mp_holistic.Holistic(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=2,\n",
    "            enable_segmentation=False,\n",
    "            refine_face_landmarks=True\n",
    "        )\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        keypoints_list = []\n",
    "\n",
    "        frame_count = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "            # Преобразование изображения в формат RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Получение результатов\n",
    "            results = holistic.process(image)\n",
    "\n",
    "            keypoints = []\n",
    "\n",
    "            # Позы\n",
    "            if results.pose_landmarks:\n",
    "                for landmark in results.pose_landmarks.landmark:\n",
    "                    keypoints.extend([landmark.x, landmark.y,\n",
    "                                      landmark.z, landmark.visibility])\n",
    "            else:\n",
    "                keypoints.extend([0] * 33 * 4)\n",
    "\n",
    "            # Лицо\n",
    "            if results.face_landmarks:\n",
    "                for landmark in results.face_landmarks.landmark:\n",
    "                    keypoints.extend([landmark.x, landmark.y, landmark.z])\n",
    "            else:\n",
    "                keypoints.extend([0] * 468 * 3)\n",
    "\n",
    "            # Левая рука\n",
    "            if results.left_hand_landmarks:\n",
    "                for landmark in results.left_hand_landmarks.landmark:\n",
    "                    keypoints.extend([landmark.x, landmark.y, landmark.z])\n",
    "            else:\n",
    "                keypoints.extend([0] * 21 * 3)\n",
    "\n",
    "            # Правая рука\n",
    "            if results.right_hand_landmarks:\n",
    "                for landmark in results.right_hand_landmarks.landmark:\n",
    "                    keypoints.extend([landmark.x, landmark.y, landmark.z])\n",
    "            else:\n",
    "                keypoints.extend([0] * 21 * 3)\n",
    "\n",
    "            keypoints_list.append(keypoints)\n",
    "\n",
    "            if visualize and frame_count == 1:\n",
    "                # Визуализация ключевых точек на первом кадре\n",
    "                annotated_image = frame.copy()\n",
    "                # Позы\n",
    "                if results.pose_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        annotated_image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "                # Лицо\n",
    "                if results.face_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        annotated_image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)\n",
    "                # Левая рука\n",
    "                if results.left_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        annotated_image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                # Правая рука\n",
    "                if results.right_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        annotated_image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                # Преобразование изображения в формат RGB для отображения\n",
    "                annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "                # Отображение изображения в Jupyter Notebook\n",
    "                plt.figure(figsize=(10, 10))\n",
    "                plt.imshow(annotated_image)\n",
    "                plt.axis('off')\n",
    "                plt.title(f\"Ключевые точки для видео: {os.path.basename(video_path)}\")\n",
    "                plt.show()\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        holistic.close()\n",
    "\n",
    "        # Усредняем ключевые точки по всем кадрам\n",
    "        keypoints_array = np.array(keypoints_list)\n",
    "        if keypoints_array.size == 0:\n",
    "            num_keypoints = (33 * 4) + (468 * 3) + (21 * 3 * 2)\n",
    "            keypoints_mean = np.zeros(num_keypoints)\n",
    "        else:\n",
    "            keypoints_mean = keypoints_array.mean(axis=0)\n",
    "\n",
    "        return keypoints_mean\n",
    "\n",
    "# Путь к CSV-файлу с аннотациями и к директории с видео\n",
    "csv_file = 'processed_data/train.csv'\n",
    "path_to_video = 'processed_data/train_video'\n",
    "\n",
    "# Создание экземпляра датасета\n",
    "dataset = PersonalityDataset(csv_file, path_to_video)\n",
    "\n",
    "# Списки для хранения признаков, меток и идентификаторов видео\n",
    "X = []\n",
    "y = []\n",
    "video_ids = []\n",
    "\n",
    "N_visualization_samples = 5  # Количество видео для визуализации\n",
    "\n",
    "for idx in range(len(dataset)):\n",
    "    row = dataset.data.iloc[idx]\n",
    "    video_file = row['video_file']\n",
    "    video_path = os.path.join(path_to_video, video_file)\n",
    "    if not os.path.isfile(video_path):\n",
    "        print(f\"Video file not found: {video_path}\")\n",
    "        continue\n",
    "\n",
    "    visualize = idx < N_visualization_samples\n",
    "\n",
    "    # Извлечение ключевых точек с визуализацией\n",
    "    keypoints = dataset.extract_keypoints(video_path, visualize=visualize)\n",
    "\n",
    "    # Подготовка меток\n",
    "    labels = {}\n",
    "    for label_name in label_names:\n",
    "        labels[label_name] = torch.tensor(row[label_name], dtype=torch.float32)\n",
    "\n",
    "    # Извлекаем значения меток\n",
    "    label_values = []\n",
    "    for label_name in label_names:\n",
    "        label_value = labels[label_name].item()\n",
    "        label_values.append(label_value)\n",
    "\n",
    "    X.append(keypoints)\n",
    "    y.append(label_values)\n",
    "    video_ids.append(video_file)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "video_ids = np.array(video_ids)\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test, video_ids_train, video_ids_test = train_test_split(\n",
    "    X, y, video_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Обучение моделей\n",
    "n_labels = y_train.shape[1]\n",
    "regressors = {}\n",
    "\n",
    "for i in range(n_labels):\n",
    "    print(f\"Обучение модели для метки '{label_names[i]}' ({i+1}/{n_labels})\")\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=1000,\n",
    "        learning_rate=0.01,\n",
    "        depth=6,\n",
    "        loss_function='RMSE',\n",
    "        eval_metric='RMSE',\n",
    "        random_seed=42,\n",
    "        verbose=100\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train[:, i],\n",
    "        eval_set=(X_test, y_test[:, i]),\n",
    "        use_best_model=True\n",
    "    )\n",
    "    regressors[label_names[i]] = model\n",
    "\n",
    "# Предсказания и оценка\n",
    "predictions = {label_name: {} for label_name in label_names}\n",
    "ground_truths = {label_name: {} for label_name in label_names}\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test[i]\n",
    "    video_id = video_ids_test[i]\n",
    "\n",
    "    for idx, label_name in enumerate(label_names):\n",
    "        model = regressors[label_name]\n",
    "        y_pred = model.predict(x.reshape(1, -1))[0]\n",
    "        y_true = y_test[i][idx]\n",
    "\n",
    "        predictions[label_name][video_id] = y_pred\n",
    "        ground_truths[label_name][video_id] = y_true\n",
    "\n",
    "# Функции для вычисления метрики F1 score\n",
    "def calculate_tag_f1(pred, truth, epsilon=0.1):\n",
    "    predicts = []\n",
    "    truths_list = []\n",
    "    for key in truth.keys():\n",
    "        pred_value = pred.get(key)\n",
    "        if pred_value is None:\n",
    "            raise Exception(f\"Предсказание для видео {key} отсутствует\")\n",
    "        truth_value = truth[key]\n",
    "        predicts.append(1 if abs(pred_value - truth_value) < epsilon else 0)\n",
    "        truths_list.append(1)  # Истинные значения всегда 1\n",
    "    return f1_score(truths_list, predicts, average='macro')\n",
    "\n",
    "def calculate_f1(predictions, ground_truths):\n",
    "    results = []\n",
    "    for label_name in label_names:\n",
    "        pred = predictions[label_name]\n",
    "        truth = ground_truths[label_name]\n",
    "        f1 = calculate_tag_f1(pred, truth)\n",
    "        results.append(f1)\n",
    "        print(f\"F1 score для метки '{label_name}': {f1:.4f}\")\n",
    "    average_f1 = np.mean(results)\n",
    "    print(f\"\\nСредний F1 score по всем меткам: {average_f1:.4f}\")\n",
    "    return average_f1\n",
    "\n",
    "average_f1 = calculate_f1(predictions, ground_truths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c2f24-bf64-458c-be82-49e5594d9070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
